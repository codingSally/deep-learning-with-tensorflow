{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5模型：卷积层 - 池化层 - 卷积层 - 池化层 - 全连接层 - 全连接层 - 全连接层\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 损失函数和方向传播，都可以复用第五章全连接网络的计算。唯一的区别在于卷积神经网络的输入层为一个三维矩阵，所以需要调整输入数据的格式\n",
    "\n",
    "# 在第五章全连接网络识别手写数据基础上修改\n",
    "# 调整输入数据placeholder 的格式，输入为一个四维矩阵\n",
    "x = tf.placeholder(tf.float32, [\n",
    "    BTACH_SIZE,\n",
    "    mnist_inference.IMAGE_SIZE,\n",
    "    mnist_inference.IMAGE_SIZE,\n",
    "    mnist_inference.NUM_CHANNELS],\n",
    "name = 'x-input')\n",
    "\n",
    "# 类似地将输入的训练数据格式调整为一个四维矩阵，并将这个调整后的数据传入sess.run过程\n",
    "reshaped_xs = np.reshape(xs, [BTACH_SIZE,\n",
    "    mnist_inference.IMAGE_SIZE,\n",
    "    mnist_inference.IMAGE_SIZE,\n",
    "    mnist_inference.NUM_CHANNELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LeNet-5模型示例\n",
    "import tensorflow as tf \n",
    "\n",
    "# 配置神经网络的参数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "INMAE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# 第一层卷积层的尺寸和深度\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "# 第二层卷积层的尺寸和深度\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "# 全连接层的节点个数\n",
    "FC_SIZE = 512\n",
    "\n",
    "# 定义卷积神经网络的前向传播\n",
    "# 程序中使用了dropout防止过拟合\n",
    " def inference(input_tensor, train, regularizer):\n",
    "        # 声明第一卷积层的变量并实现前向传播过程\n",
    "        # 通过使用不同的命名空间来隔离不同层的变量，这可以让每一层中变量命名只需要考虑当前作用层的作用，而不需要担心重名的问题\n",
    "        # 输入为28*28*1， 全0填充，所以输出为28*28*32\n",
    "        with tf.variable_scope('layer1-conv1'):\n",
    "            conv1_weights = tf.get_variable(\n",
    "                # 四维矩阵前两维代表了过滤器的尺寸，第三维表示当前层的深度，第四维度表示过滤器的深度\n",
    "                'weights', [ CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "                 initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "            conv1_biases = tf.get_variable(\n",
    "                'bias', [CONV1_DEEP], \n",
    "                 initializer = tf.constant_initializer(0.0))\n",
    "            \n",
    "            # 使用边长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充\n",
    "            conv1 = tf.nn.conv2d(\n",
    "                 input_tensor, conv1_weights, strides=[1,1,1,1], padding='SAME')\n",
    "            \n",
    "            relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "            \n",
    "        # 实现第二层池化层的前向传播过程\n",
    "        # 这里使用最大池化层\n",
    "        # 这一层的输入就是上一层的输出，即28*28*32，输出为14*14*32\n",
    "        with tf.name_scope('layer2-pool1'):\n",
    "            pool1 = tf.nn.max_pool(\n",
    "                relu1, ksize = [1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "        \n",
    "        # 声明第三层卷积层的变量并实现前向传播过程\n",
    "        # 这一层的输入为14*14*32， 输出为14*14*64的矩阵\n",
    "        with tf.variable_scope('layer3-conv2'):\n",
    "            conv2_weights = tf.get_variable(\n",
    "                # 四维矩阵前两维代表了过滤器的尺寸，第三维表示当前层的深度，第四维度表示过滤器的深度\n",
    "                'weights', [ CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "                 initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "            conv2_biases = tf.get_variable(\n",
    "                'bias', [CONV2_DEEP], \n",
    "                 initializer = tf.constant_initializer(0.0))\n",
    "            \n",
    "            # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1，且使用全0填充\n",
    "            conv2 = tf.nn.conv2d(\n",
    "                 pool1, conv2_weights, strides=[1,1,1,1], padding='SAME')\n",
    "            \n",
    "            relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "            \n",
    "        # 实现第四层池化层的前向传播过程，这一层和第二层的结构一样\n",
    "        # 这一层的输入为14*14*64， 输出为7*7*64的矩阵\n",
    "        with tf.name_scope('layer4-pool2'):\n",
    "            pool2 = tf.nn.max_pool(\n",
    "                relu2, ksize = [1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "            \n",
    "        # 将第四层池化层的输出转换为第五层全连接层的输入格式。\n",
    "        # 第四层的输出为7*7*64的矩阵，然而第五层全连接层需要的输入格式为向量，所以在这里需要将这个7*7*64的矩阵拉直成一个向量\n",
    "        pool_shape = pool2.get_shape().aslist()\n",
    "        \n",
    "        # 计算将矩阵拉直成向量之后的长度，这个长度就是矩阵长宽及深度的乘积。\n",
    "        # 注意这里pool_shape[0]为一个batch中数据的个数\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        \n",
    "        # 通过tf.reshape函数将第四层的输出变成一个batch的向量\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "        \n",
    "        # 声明第五层全连接层的变量并实现前向传播过程。\n",
    "        # 这一层是拉直之后的一组向量，向量长度为3136，输出是一组长度为512的向量\n",
    "        # 该层使用dropout正则化避免过拟合，dropout一般只在全连接层而不是卷积层或池化层使用\n",
    "        with tf.variable_scope('layer5-fc1'):\n",
    "            fc1_weights = tf.get_variable(\n",
    "                'weights', [ nodes, FC_SIZE],\n",
    "                initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "            # 只有全连接层的权重需要加入正则化\n",
    "            if regularizer != None:\n",
    "                tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "            fc1_biases = f.get_variable(\n",
    "                'bias', [FC_SIZE], \n",
    "                 initializer = tf.constant_initializer(0.1))\n",
    "            \n",
    "            fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "            if train : fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "                \n",
    "        # 声明第六层全连接层的变量并实现前向传播过程。\n",
    "        # 这一层输入为一组长度为512的向量，输出为一组长度为10的向量\n",
    "        # 这一层的输出通过Softmax之后就得到了最后的分类结果\n",
    "        with tf.variable_scope('layer6-fc2'):\n",
    "            fc2_weights = tf.get_variable(\n",
    "                'weights', [FC_SIZE, NUM_LABELS],\n",
    "                initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "            # 只有全连接层的权重需要加入正则化\n",
    "            if regularizer != None:\n",
    "                tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "            fc2_biases = f.get_variable(\n",
    "                'bias', [NUM_LABELS], \n",
    "                 initializer = tf.constant_initializer(0.1))\n",
    "            \n",
    "            logit = tf.matmul(fc1, fc2_weights) + fc1_biases\n",
    "            \n",
    "        # 返回第六层的输出\n",
    "        return logit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
